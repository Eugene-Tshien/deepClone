{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhY161um5GcF"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ML-KULeuven/deepstochlog-lm.git\n",
        "%cd deepstochlog-lm/\n",
        "!cp -r data/lms_task2 /content/\n",
        "!ls /content/lms_task2\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "QnMdIT-jxrJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZn33NlIqxkB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gc\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "from torch import cuda\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZTasLoR4Wez"
      },
      "outputs": [],
      "source": [
        "class SQLDataset(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "          self, dataframe, tokenizer, source_len, target_len, source_text, target_text, padding\n",
        "  ):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.target_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "    self.padding = padding\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    source_text = \" \".join(source_text.split())\n",
        "    target_text = \" \".join(target_text.split())\n",
        "\n",
        "    if self.padding:\n",
        "      source = self.tokenizer.batch_encode_plus(\n",
        "        [source_text],\n",
        "        max_length=self.source_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "      target = self.tokenizer.batch_encode_plus(\n",
        "        [target_text],\n",
        "        max_length=self.target_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "    else:\n",
        "      source = self.tokenizer.batch_encode_plus(\n",
        "        [source_text],\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "      target = self.tokenizer.batch_encode_plus(\n",
        "        [target_text],\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "    source_ids = source[\"input_ids\"].squeeze()\n",
        "    source_mask = source[\"attention_mask\"].squeeze()\n",
        "    target_ids = target[\"input_ids\"].squeeze()\n",
        "    target_mask = target[\"attention_mask\"].squeeze()\n",
        "\n",
        "    return {\n",
        "      \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "      \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "      \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "      \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPh-AdCj4bVd"
      },
      "outputs": [],
      "source": [
        "def train(epoch, fold, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "      y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "      y_ids = y[:, :-1].contiguous()\n",
        "      lm_labels = y[:, 1:].clone().detach()\n",
        "      lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "      ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "      mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "      # print(lm_labels)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=ids,\n",
        "        attention_mask=mask,\n",
        "        labels=y_ids,\n",
        "      )\n",
        "      loss = outputs[0]\n",
        "\n",
        "      # if _ % 10 == 0:\n",
        "      #   training_logger.add_row(str(epoch), str(fold), str(_), str(loss))\n",
        "      #   console.print(training_logger)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaVJcFJF4gYO"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, fold, tokenizer, model, device, loader, target_len, save_pred):\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  sample_no = 0\n",
        "  acc = 0\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask,\n",
        "              max_length=target_len,\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          sample_no = len(target)\n",
        "          for i in range(len(preds)):\n",
        "            if preds[i] == target[i]:\n",
        "              acc += 1\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "\n",
        "  if save_pred:\n",
        "    final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "    #replace with you output dir\n",
        "    prediction_dir = \"output/prediction.csv\"\n",
        "    final_df.to_csv(prediction_dir)\n",
        "\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQXEg-Pa4kBR"
      },
      "outputs": [],
      "source": [
        "def Trainer(\n",
        "    train_dataset=None, val_dataset=None, test_dataset=None, model_params=None, model_number = -1, output_dir=None, save_pred=False\n",
        "  ):\n",
        "\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    np.random.seed(model_params[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "    padding = True\n",
        "\n",
        "    training_set = SQLDataset(\n",
        "      train_dataset,\n",
        "      tokenizer,\n",
        "      model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "      model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "      \"source\",\n",
        "      \"target\",\n",
        "      padding,\n",
        "    )\n",
        "\n",
        "    train_params = {\n",
        "      \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      \"shuffle\": True,\n",
        "      \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "    if val_dataset is not None:\n",
        "      val_set = SQLDataset(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        \"source\",\n",
        "        \"target\",\n",
        "        padding,\n",
        "      )\n",
        "\n",
        "      val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "      }\n",
        "\n",
        "      val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    if test_dataset is not None:\n",
        "      testing_set = SQLDataset(\n",
        "          test_dataset,\n",
        "          tokenizer,\n",
        "          model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "          model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "          \"source\",\n",
        "          \"target\",\n",
        "          padding,\n",
        "      )\n",
        "\n",
        "      test_params = {\n",
        "        \"batch_size\": model_params[\"TEST_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "      }\n",
        "\n",
        "      testing_loader = DataLoader(testing_set, **test_params)\n",
        "\n",
        "\n",
        "    val_acc_all = []\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "\n",
        "      optimizer = torch.optim.Adam(params=model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "      train(epoch, model_number, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "      if val_dataset is not None:\n",
        "        val_acc = validate(epoch, model_number, tokenizer, model, device, val_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"], save_pred)\n",
        "        val_acc = val_acc / len(val_dataset)\n",
        "        val_acc_all.append(val_acc)\n",
        "        print(epoch, model_number, val_acc)\n",
        "\n",
        "    if test_dataset is not None:\n",
        "      test_acc = validate(epoch, model_number, tokenizer, model, device, testing_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"], save_pred)\n",
        "      print(test_acc / len(test_dataset))\n",
        "\n",
        "\n",
        "    if output_dir is not None:\n",
        "      console.log(f\"[Saving Model]...\\n\")\n",
        "\n",
        "      path = os.path.join(output_dir, str(model_params[\"TRAIN_BATCH_SIZE\"]) + \"_\" + str(model_params[\"TRAIN_EPOCHS\"]) + \"_\" + str(model_params[\"LEARNING_RATE\"]))\n",
        "      model.save_pretrained(path)\n",
        "      tokenizer.save_pretrained(path)\n",
        "\n",
        "    return val_acc_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8dXuBw1RfjJ"
      },
      "outputs": [],
      "source": [
        "def cross_validation(k=5, model_params = None, df_data=None):\n",
        "\n",
        "  val_size = math.ceil(model_params[\"TRAIN_NO\"]/k)\n",
        "  full_train = df_data[:model_params[\"TRAIN_NO\"]]\n",
        "\n",
        "  val_all = []\n",
        "  for i in range(k):\n",
        "    if (i+1)*val_size <= model_params[\"TRAIN_NO\"]:\n",
        "      val_dataset = full_train[i*val_size:(i+1)*val_size]\n",
        "    else:\n",
        "      val_dataset = full_train[i*val_size:]\n",
        "    train_dataset = full_train.drop(val_dataset.index).reset_index(drop=True)\n",
        "    val_dataset = val_dataset.reset_index(drop=True)\n",
        "\n",
        "    val = Trainer(\n",
        "                            train_dataset = train_dataset,\n",
        "                            val_dataset = val_dataset,\n",
        "                            model_params=model_params,\n",
        "                            model_number = i,\n",
        "                        )\n",
        "    val_all.append(val)\n",
        "\n",
        "  avg_val = []\n",
        "  for i in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "    total_val = 0\n",
        "    for array in val_all:\n",
        "        total_val += array[i]\n",
        "    avg_val.append(total_val / k)\n",
        "\n",
        "  print(\"Model avg\")\n",
        "  print(avg_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def full_train(model_params=None, df_data=None, task=None, save_model=False, save_pred=False):\n",
        "  train_dataset = df_data[:model_params[\"TRAIN_NO\"]]\n",
        "  test_dataset = df_data.drop(train_dataset.index).reset_index(drop=True)\n",
        "  if save_model:\n",
        "    output_dir = \"lms/\"+task\n",
        "  else:\n",
        "    output_dir = None\n",
        "  Trainer(\n",
        "            train_dataset = train_dataset,\n",
        "            test_dataset = test_dataset,\n",
        "            model_params=model_params,\n",
        "            output_dir= output_dir,\n",
        "            save_pred = save_pred,\n",
        "  )"
      ],
      "metadata": {
        "id": "qp2iElI5g6cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgVCcIBGyuiK"
      },
      "outputs": [],
      "source": [
        "def load_data(task=\"table\"):\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  # replace \"root\" with you data root\n",
        "  if task == \"t5_baseline\":\n",
        "    train_dir = \"train_\"+task+\".json\"\n",
        "    test_dir = \"test_\"+task+\".json\"\n",
        "    root = \"\"\n",
        "  else:\n",
        "    train_dir = \"train_\"+task+\"_task2.json\"\n",
        "    test_dir = \"test_\"+task+\"_task2.json\"\n",
        "    root = \"lms_task2/\"\n",
        "\n",
        "  with open(root + train_dir, \"r\") as file:\n",
        "      train_data = json.load(file)\n",
        "  train_len = len(train_data)\n",
        "  with open(root + test_dir, \"r\") as file:\n",
        "      test_data = json.load(file)\n",
        "  data = train_data + test_data\n",
        "\n",
        "  tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "  source_len = []\n",
        "  target_len = []\n",
        "  for sample in data:\n",
        "      prompt = sample[\"prompt\"]\n",
        "      target = sample[\"target\"]\n",
        "      source_len.append(tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].size()[1])\n",
        "      target_len.append(tokenizer(target, return_tensors=\"pt\")[\"input_ids\"].size()[1])\n",
        "      dataset.append([prompt, target])\n",
        "  max_source_len = max(source_len)\n",
        "  max_target_len = max(target_len)\n",
        "  if max_source_len > 512:\n",
        "    max_source_len = 512\n",
        "  print(max_source_len)\n",
        "  print(max_target_len)\n",
        "\n",
        "  df_data = pd.DataFrame(dataset, columns = ['source', 'target'])\n",
        "  print(len(train_data))\n",
        "  print(len(test_data))\n",
        "\n",
        "  return max_source_len, max_target_len, df_data, train_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxM9vm6eeznC"
      },
      "outputs": [],
      "source": [
        "console = Console(record=True)\n",
        "\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Fold\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# \"column_cf\" for T5-small + CFG baseline, \"t5_baseline\" for vanilla T5-small baseline\n",
        "# tasks = [\"table\", \"column\", \"column_cf\", \"type\", \"where\", \"group by\", \"order by\", \"ss\", \"having\", \"limit\", \"op\", \"desc\", \"t5_baseline\"]\n",
        "task = \"table\"\n",
        "# put lms_task2 folder in git repo \"/data\" to session storage or your google drive\n",
        "source_len, target_len, df_data, train_len = load_data(task)\n",
        "epoch = 10\n",
        "\n",
        "for batch in [8, 16, 32, 64, 128]:\n",
        "  for lr in [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]:\n",
        "    print(\"this is for\")\n",
        "    print(batch, lr)\n",
        "    model_params = {\n",
        "      \"MODEL\": \"t5-small\",\n",
        "      \"TRAIN_BATCH_SIZE\": batch,\n",
        "      \"VALID_BATCH_SIZE\": 8,\n",
        "      \"TEST_BATCH_SIZE\": 8,\n",
        "      \"TRAIN_EPOCHS\": epoch,\n",
        "      \"VAL_EPOCHS\": 1,\n",
        "      \"LEARNING_RATE\": lr,\n",
        "      \"MAX_SOURCE_TEXT_LENGTH\": source_len,\n",
        "      \"MAX_TARGET_TEXT_LENGTH\": target_len,\n",
        "      \"SEED\": 23,\n",
        "      \"TRAIN_NO\": train_len,\n",
        "    }\n",
        "    cross_validation(k=5, model_params=model_params, df_data=df_data)\n",
        "\n",
        "# fill in chosen epoch, batch_size, lr from cross_validation\n",
        "chosen_model_params = {\n",
        "    \"MODEL\": \"t5-small\",\n",
        "    \"TRAIN_BATCH_SIZE\": batch,\n",
        "    \"VALID_BATCH_SIZE\": 8,\n",
        "    \"TEST_BATCH_SIZE\": 8,\n",
        "    \"TRAIN_EPOCHS\": epoch,\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    \"LEARNING_RATE\": lr,\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": source_len,\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": target_len,\n",
        "    \"SEED\": 23,\n",
        "    \"TRAIN_NO\": train_len,\n",
        "}\n",
        "full_train(model_params=chosen_model_params, df_data=df_data, task=task, save_model=True, save_pred=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}