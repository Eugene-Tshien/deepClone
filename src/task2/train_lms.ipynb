{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FhY161um5GcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0612464c-a156-4023-8120-925847b30dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ML-KULeuven/deepstochlog-lm.git\n",
        "%cd deepstochlog-lm/\n",
        "!cp -r data/lms_task2 /content/\n",
        "!ls /content/lms_task2\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "QnMdIT-jxrJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d033a3-cdfa-4def-8db6-5341c6434a73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepstochlog-lm'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 141 (delta 37), reused 121 (delta 29), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (141/141), 3.47 MiB | 7.54 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "/content/deepstochlog-lm\n",
            " test_column_cf_task2.json   train_column_cf_task2.json\n",
            " test_column_task2.json      train_column_task2.json\n",
            " test_desc_task2.json\t     train_desc_task2.json\n",
            "'test_group by_task2.json'  'train_group by_task2.json'\n",
            " test_having_task2.json      train_having_task2.json\n",
            " test_limit_task2.json\t     train_limit_task2.json\n",
            " test_op_task2.json\t     train_op_task2.json\n",
            "'test_order by_task2.json'  'train_order by_task2.json'\n",
            " test_ss_task2.json\t     train_ss_task2.json\n",
            " test_table_task2.json\t     train_table_task2.json\n",
            " test_type_task2.json\t     train_type_task2.json\n",
            " test_where_task2.json\t     train_where_task2.json\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uZn33NlIqxkB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gc\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "from torch import cuda\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AZTasLoR4Wez"
      },
      "outputs": [],
      "source": [
        "class SQLDataset(Dataset):\n",
        "\n",
        "  def __init__(\n",
        "          self, dataframe, tokenizer, source_len, target_len, source_text, target_text, padding\n",
        "  ):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.target_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "    self.padding = padding\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    source_text = \" \".join(source_text.split())\n",
        "    target_text = \" \".join(target_text.split())\n",
        "\n",
        "    if self.padding:\n",
        "      source = self.tokenizer.batch_encode_plus(\n",
        "        [source_text],\n",
        "        max_length=self.source_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "      target = self.tokenizer.batch_encode_plus(\n",
        "        [target_text],\n",
        "        max_length=self.target_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "    else:\n",
        "      source = self.tokenizer.batch_encode_plus(\n",
        "        [source_text],\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "      target = self.tokenizer.batch_encode_plus(\n",
        "        [target_text],\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "    source_ids = source[\"input_ids\"].squeeze()\n",
        "    source_mask = source[\"attention_mask\"].squeeze()\n",
        "    target_ids = target[\"input_ids\"].squeeze()\n",
        "    target_mask = target[\"attention_mask\"].squeeze()\n",
        "\n",
        "    return {\n",
        "      \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "      \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "      \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "      \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TPh-AdCj4bVd"
      },
      "outputs": [],
      "source": [
        "def train(epoch, fold, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "      y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "      y_ids = y[:, :-1].contiguous()\n",
        "      lm_labels = y[:, 1:].clone().detach()\n",
        "      lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "      ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "      mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "      # print(lm_labels)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=ids,\n",
        "        attention_mask=mask,\n",
        "        labels=y_ids,\n",
        "      )\n",
        "      loss = outputs[0]\n",
        "\n",
        "      # if _ % 10 == 0:\n",
        "      #   training_logger.add_row(str(epoch), str(fold), str(_), str(loss))\n",
        "      #   console.print(training_logger)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xaVJcFJF4gYO"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, fold, tokenizer, model, device, loader, target_len, save_pred):\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  sample_no = 0\n",
        "  acc = 0\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask,\n",
        "              max_length=target_len,\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          sample_no = len(target)\n",
        "          for i in range(len(preds)):\n",
        "            if preds[i] == target[i]:\n",
        "              acc += 1\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "\n",
        "  if save_pred:\n",
        "    final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "    #replace with you output dir\n",
        "    prediction_dir = \"output/prediction.csv\"\n",
        "    final_df.to_csv(prediction_dir)\n",
        "\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EQXEg-Pa4kBR"
      },
      "outputs": [],
      "source": [
        "def Trainer(\n",
        "    train_dataset=None, val_dataset=None, test_dataset=None, model_params=None, model_number = -1, output_dir=None, save_pred=False\n",
        "  ):\n",
        "\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    np.random.seed(model_params[\"SEED\"])\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "    padding = True\n",
        "\n",
        "    training_set = SQLDataset(\n",
        "      train_dataset,\n",
        "      tokenizer,\n",
        "      model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "      model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "      \"source\",\n",
        "      \"target\",\n",
        "      padding,\n",
        "    )\n",
        "\n",
        "    train_params = {\n",
        "      \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      \"shuffle\": True,\n",
        "      \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "    if val_dataset is not None:\n",
        "      val_set = SQLDataset(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        \"source\",\n",
        "        \"target\",\n",
        "        padding,\n",
        "      )\n",
        "\n",
        "      val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "      }\n",
        "\n",
        "      val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    if test_dataset is not None:\n",
        "      testing_set = SQLDataset(\n",
        "          test_dataset,\n",
        "          tokenizer,\n",
        "          model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "          model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "          \"source\",\n",
        "          \"target\",\n",
        "          padding,\n",
        "      )\n",
        "\n",
        "      test_params = {\n",
        "        \"batch_size\": model_params[\"TEST_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "      }\n",
        "\n",
        "      testing_loader = DataLoader(testing_set, **test_params)\n",
        "\n",
        "\n",
        "    val_acc_all = []\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "\n",
        "      optimizer = torch.optim.Adam(params=model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "      train(epoch, model_number, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "      if val_dataset is not None:\n",
        "        val_acc = validate(epoch, model_number, tokenizer, model, device, val_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"], save_pred)\n",
        "        val_acc = val_acc / len(val_dataset)\n",
        "        val_acc_all.append(val_acc)\n",
        "        print(epoch, model_number, val_acc)\n",
        "\n",
        "    if test_dataset is not None:\n",
        "      test_acc = validate(epoch, model_number, tokenizer, model, device, testing_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"], save_pred)\n",
        "      print(test_acc / len(test_dataset))\n",
        "\n",
        "\n",
        "    if output_dir is not None:\n",
        "      console.log(f\"[Saving Model]...\\n\")\n",
        "\n",
        "      path = os.path.join(output_dir, str(model_params[\"TRAIN_BATCH_SIZE\"]) + \"_\" + str(model_params[\"TRAIN_EPOCHS\"]) + \"_\" + str(model_params[\"LEARNING_RATE\"]))\n",
        "      model.save_pretrained(path)\n",
        "      tokenizer.save_pretrained(path)\n",
        "\n",
        "    return val_acc_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l8dXuBw1RfjJ"
      },
      "outputs": [],
      "source": [
        "def cross_validation(k=5, model_params = None, df_data=None):\n",
        "\n",
        "  val_size = math.ceil(model_params[\"TRAIN_NO\"]/k)\n",
        "  full_train = df_data[:model_params[\"TRAIN_NO\"]]\n",
        "\n",
        "  val_all = []\n",
        "  for i in range(k):\n",
        "    if (i+1)*val_size <= model_params[\"TRAIN_NO\"]:\n",
        "      val_dataset = full_train[i*val_size:(i+1)*val_size]\n",
        "    else:\n",
        "      val_dataset = full_train[i*val_size:]\n",
        "    train_dataset = full_train.drop(val_dataset.index).reset_index(drop=True)\n",
        "    val_dataset = val_dataset.reset_index(drop=True)\n",
        "\n",
        "    val = Trainer(\n",
        "                            train_dataset = train_dataset,\n",
        "                            val_dataset = val_dataset,\n",
        "                            model_params=model_params,\n",
        "                            model_number = i,\n",
        "                        )\n",
        "    val_all.append(val)\n",
        "\n",
        "  avg_val = []\n",
        "  for i in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "    total_val = 0\n",
        "    for array in val_all:\n",
        "        total_val += array[i]\n",
        "    avg_val.append(total_val / k)\n",
        "\n",
        "  print(\"Model avg\")\n",
        "  print(avg_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def full_train(model_params=None, df_data=None, task=None, save_model=False, save_pred=False):\n",
        "  train_dataset = df_data[:model_params[\"TRAIN_NO\"]]\n",
        "  test_dataset = df_data.drop(train_dataset.index).reset_index(drop=True)\n",
        "  if save_model:\n",
        "    output_dir = \"lms/\"+task\n",
        "  else:\n",
        "    output_dir = None\n",
        "  Trainer(\n",
        "            train_dataset = train_dataset,\n",
        "            test_dataset = test_dataset,\n",
        "            model_params=model_params,\n",
        "            output_dir= output_dir,\n",
        "            save_pred = save_pred,\n",
        "  )"
      ],
      "metadata": {
        "id": "qp2iElI5g6cO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UgVCcIBGyuiK"
      },
      "outputs": [],
      "source": [
        "def load_data(task=\"table\"):\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  # replace \"root\" with you data root\n",
        "  if task == \"t5_baseline\":\n",
        "    train_dir = \"train_\"+task+\".json\"\n",
        "    test_dir = \"test_\"+task+\".json\"\n",
        "    root = \"\"\n",
        "  else:\n",
        "    train_dir = \"train_\"+task+\"_task2.json\"\n",
        "    test_dir = \"test_\"+task+\"_task2.json\"\n",
        "    root = \"lms_task2/\"\n",
        "\n",
        "  with open(root + train_dir, \"r\") as file:\n",
        "      train_data = json.load(file)\n",
        "  train_len = len(train_data)\n",
        "  with open(root + test_dir, \"r\") as file:\n",
        "      test_data = json.load(file)\n",
        "  data = train_data + test_data\n",
        "\n",
        "  tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "  source_len = []\n",
        "  target_len = []\n",
        "  for sample in data:\n",
        "      prompt = sample[\"prompt\"]\n",
        "      target = sample[\"target\"]\n",
        "      source_len.append(tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].size()[1])\n",
        "      target_len.append(tokenizer(target, return_tensors=\"pt\")[\"input_ids\"].size()[1])\n",
        "      dataset.append([prompt, target])\n",
        "  max_source_len = max(source_len)\n",
        "  max_target_len = max(target_len)\n",
        "  if max_source_len > 512:\n",
        "    max_source_len = 512\n",
        "  print(max_source_len)\n",
        "  print(max_target_len)\n",
        "\n",
        "  df_data = pd.DataFrame(dataset, columns = ['source', 'target'])\n",
        "  print(len(train_data))\n",
        "  print(len(test_data))\n",
        "\n",
        "  return max_source_len, max_target_len, df_data, train_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zxM9vm6eeznC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca1f56ff-4609-474c-9159-ef133c810535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n",
            "2\n",
            "417\n",
            "38\n",
            "this is for\n",
            "32 0.0005\n",
            "0 0 0.5357142857142857\n",
            "1 0 0.47619047619047616\n",
            "2 0 0.7976190476190477\n",
            "3 0 0.8452380952380952\n",
            "4 0 0.9047619047619048\n",
            "5 0 0.9047619047619048\n",
            "6 0 0.9047619047619048\n",
            "7 0 0.9047619047619048\n",
            "8 0 0.9166666666666666\n",
            "9 0 0.9166666666666666\n",
            "0 1 0.38095238095238093\n",
            "1 1 0.6904761904761905\n",
            "2 1 0.7619047619047619\n",
            "3 1 0.9523809523809523\n",
            "4 1 0.9166666666666666\n",
            "5 1 0.9642857142857143\n",
            "6 1 0.9761904761904762\n",
            "7 1 0.9523809523809523\n",
            "8 1 0.9642857142857143\n",
            "9 1 0.9642857142857143\n",
            "0 2 0.5357142857142857\n",
            "1 2 0.6428571428571429\n",
            "2 2 0.8571428571428571\n",
            "3 2 0.8571428571428571\n",
            "4 2 0.9166666666666666\n",
            "5 2 0.9166666666666666\n",
            "6 2 0.9166666666666666\n",
            "7 2 0.9404761904761905\n",
            "8 2 0.9404761904761905\n",
            "9 2 0.9285714285714286\n",
            "0 3 0.5476190476190477\n",
            "1 3 0.6309523809523809\n",
            "2 3 0.6071428571428571\n",
            "3 3 0.8452380952380952\n",
            "4 3 0.9047619047619048\n",
            "5 3 0.8928571428571429\n",
            "6 3 0.9285714285714286\n",
            "7 3 0.9166666666666666\n",
            "8 3 0.9642857142857143\n",
            "9 3 0.9523809523809523\n",
            "0 4 0.4691358024691358\n",
            "1 4 0.9259259259259259\n",
            "2 4 0.9382716049382716\n",
            "3 4 0.9382716049382716\n",
            "4 4 0.9506172839506173\n",
            "5 4 0.8888888888888888\n",
            "6 4 0.9629629629629629\n",
            "7 4 0.9629629629629629\n",
            "8 4 0.9506172839506173\n",
            "9 4 0.9629629629629629\n",
            "Model avg\n",
            "[0.49382716049382713, 0.6732804232804233, 0.7924162257495591, 0.8876543209876543, 0.9186948853615521, 0.9134920634920635, 0.9378306878306878, 0.9354497354497354, 0.9472663139329806, 0.9449735449735449]\n",
            "batch:  32\n",
            "lr:  0.0005\n",
            "epoch:  10\n",
            "0.9210526315789473\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[12:44:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSaving Model\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                          \u001b]8;id=894428;file:///tmp/ipython-input-709750746.py\u001b\\\u001b[2mipython-input-709750746.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=955994;file:///tmp/ipython-input-709750746.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m                                                                           \u001b[2m                             \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12:44:45] </span><span style=\"font-weight: bold\">[</span>Saving Model<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                          <a href=\"file:///tmp/ipython-input-709750746.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">ipython-input-709750746.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipython-input-709750746.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3ca9f0c5-6db8-44f3-a31a-281caf44e3bd\", \"desc.zip\", 205781779)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete\n"
          ]
        }
      ],
      "source": [
        "console = Console(record=True)\n",
        "\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Fold\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# \"column_cf\" for T5-small + CFG baseline, \"t5_baseline\" for vanilla T5-small baseline\n",
        "# tasks = [\"table\", \"column\", \"column_cf\", \"type\", \"where\", \"group by\", \"order by\", \"ss\", \"having\", \"limit\", \"op\", \"desc\", \"t5_baseline\"]\n",
        "task = \"desc\"\n",
        "# put lms_task2 folder in git repo \"/data\" to session storage or your google drive\n",
        "source_len, target_len, df_data, train_len = load_data(task)\n",
        "\n",
        "batch_size = 32\n",
        "epoch = 10\n",
        "learning_rate = 5e-4\n",
        "\n",
        "for batch in [batch_size]:\n",
        "  for lr in [learning_rate]:\n",
        "    print(\"this is for\")\n",
        "    print(batch, lr)\n",
        "    model_params = {\n",
        "      \"MODEL\": \"t5-small\",\n",
        "      \"TRAIN_BATCH_SIZE\": batch,\n",
        "      \"VALID_BATCH_SIZE\": 8,\n",
        "      \"TEST_BATCH_SIZE\": 8,\n",
        "      \"TRAIN_EPOCHS\": epoch,\n",
        "      \"VAL_EPOCHS\": 1,\n",
        "      \"LEARNING_RATE\": lr,\n",
        "      \"MAX_SOURCE_TEXT_LENGTH\": source_len,\n",
        "      \"MAX_TARGET_TEXT_LENGTH\": target_len,\n",
        "      \"SEED\": 23,\n",
        "      \"TRAIN_NO\": train_len,\n",
        "    }\n",
        "    cross_validation(k=5, model_params=model_params, df_data=df_data)\n",
        "\n",
        "# fill in chosen epoch, batch_size, lr from cross_validation\n",
        "chosen_model_params = {\n",
        "    \"MODEL\": \"t5-small\",\n",
        "    \"TRAIN_BATCH_SIZE\": batch,\n",
        "    \"VALID_BATCH_SIZE\": 8,\n",
        "    \"TEST_BATCH_SIZE\": 8,\n",
        "    \"TRAIN_EPOCHS\": epoch,\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    \"LEARNING_RATE\": lr,\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": source_len,\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": target_len,\n",
        "    \"SEED\": 23,\n",
        "    \"TRAIN_NO\": train_len,\n",
        "}\n",
        "\n",
        "print(\"batch: \", batch);\n",
        "print(\"lr: \", lr);\n",
        "print(\"epoch: \", epoch);\n",
        "\n",
        "full_train(model_params=chosen_model_params, df_data=df_data, task=task, save_model=True, save_pred=False)\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(task, 'zip', \"lms/\" + task)\n",
        "\n",
        "# Download the zip\n",
        "files.download(task + \".zip\")\n",
        "print(\"Download complete\");"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}